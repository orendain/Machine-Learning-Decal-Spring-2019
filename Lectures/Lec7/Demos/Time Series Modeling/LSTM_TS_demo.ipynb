{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we will show how vanilla Recurrent Neural Networks (RNNs) can be used to solve time-series extrapolation problems -- That is, problems that require you to predict the \"next value\" given a series of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will try to use a RNN to predict a sine wave. We create a dataset consisting of a random length of a sine wave with Gaussian noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deviation of noise we will add to our generated curves\n",
    "SIGMA = 0.02\n",
    "#Number of curves we will generate\n",
    "N = 20\n",
    "\n",
    "#dataset will hold values of all generated curves\n",
    "dataset = []\n",
    "\n",
    "for _ in range(N):\n",
    "    #Domain of our curve will be start to end\n",
    "    #Note start is also randomized now\n",
    "    start = np.random.uniform(0, 10)\n",
    "    end = np.random.uniform(40, 50)\n",
    "    #We will generate a point for all x values divisible by 0.1\n",
    "    t = np.arange(start, end, 0.1)\n",
    "    #Generate values as sin(t) + noise\n",
    "    x = np.sin(t) + np.random.normal(loc=0.0, scale=SIGMA, size=t.shape)\n",
    "    #Add generated curve to dataset\n",
    "    dataset.append(x)\n",
    "\n",
    "#Plot example curve\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(np.arange(dataset[0].shape[0]), dataset[0], s=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1/C_factor of the data will be used by context LSTM.  With C_factor = 2 the data\n",
    "#is split half and half.  Further comments will just use 2 instead of C_factor.\n",
    "C_factor = 2\n",
    "#Each 'data point' in our dataset will be ([y_0,y_1,...,y_(n-1)],[y_n/2,y_(n/2+1),...,y_n])\n",
    "#Our model will predict future values so the output will be of shifted time index.\n",
    "dataset = [\n",
    "    (torch.from_numpy(input_seq[:-1]).double(), \\\n",
    "     torch.from_numpy(input_seq[int((np.size(input_seq)-1)/C_factor)+1:]).double()) \\\n",
    "        for input_seq in dataset\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definition of our Model\n",
    "#With an LSTM we will use the first half of the input data to determine\n",
    "#the state of the curve we are working with.\n",
    "#The second half of the data will be used by a RNN to make predictions \n",
    "#considering both the data and the state computed by the LSTM.\n",
    "#Motivation: LSTM computes phase, RNN showed earlier that given phase\n",
    "#information it can appropriately model a sine wave.\n",
    "class LSTM_RNN(nn.Module):\n",
    "    #Initialize LSTM_RNN module\n",
    "    def __init__(self, hidden_size):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        #Hidden size for LSTM and RNN determined by argument\n",
    "        (self.lstm_hidden_size, self.rnn_hidden_size) = hidden_size\n",
    "        \n",
    "        #Initial state for LSTM\n",
    "        self.init_hidden = torch.nn.Parameter(torch.zeros(1,1,self.lstm_hidden_size))\n",
    "        self.init_cell = torch.nn.Parameter(torch.zeros(1,1,self.lstm_hidden_size))\n",
    "        \n",
    "        #Initialize LSTM\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=self.lstm_hidden_size)\n",
    "        \n",
    "        #Transformation matrix to be used to transform final LSTM cell state to\n",
    "        #initial RNN hidden state\n",
    "        self.state_trans = torch.nn.Parameter(torch.randn(self.lstm_hidden_size,self.rnn_hidden_size))\n",
    "        \n",
    "        #Initialize RNN\n",
    "        self.rnn = nn.RNN(input_size=1,hidden_size=self.rnn_hidden_size)\n",
    "        \n",
    "        #linear mapping used to map outputs of RNN to predictions\n",
    "        self.linear = nn.Linear(in_features=self.rnn_hidden_size, out_features=1)\n",
    "        \n",
    "    #Forward pass through model.  Returns predictions for second half of data and final hidden state.\n",
    "    def forward(self, inputs, teacher_force_prob=0.5):\n",
    "        #Number of points in passed in input\n",
    "        seq_len = inputs.shape[0]\n",
    "        #List to contain outputs of the model\n",
    "        outputs = []\n",
    "        #Length of input to be used as context, (1/2 of the data)\n",
    "        c_length = int(seq_len/C_factor)\n",
    "        context = inputs.narrow(0,0,c_length)\n",
    "        #Final cell state is used by passing the context of the input through LSTM\n",
    "        _, (_, final_cell) = self.lstm(context.reshape(c_length, 1, 1), (self.init_hidden, self.init_cell))\n",
    "        #Initial hidden state of RNN computed by transforming final cell state of LSTM\n",
    "        h = torch.matmul(final_cell, self.state_trans)\n",
    "        #For every data point in second half of input\n",
    "        for i, inp in enumerate(inputs.narrow(0,c_length, seq_len - c_length)):\n",
    "            #If it's the first point or with probability teacher_force_prob\n",
    "            #we use the actual previous value given from the data\n",
    "            #Otherwise we use what the model outputted last as input\n",
    "            if i == 0 or np.random.uniform() < teacher_force_prob:\n",
    "                inp = inp\n",
    "            else:\n",
    "                inp = output\n",
    "            #We reshape input as the RNN module expects (num_time_steps,batch_size,input_size)\n",
    "            #dimensionality of it's input.  We are manually going through one time step at a time,\n",
    "            #learning from one example at a time (batch size = 1), and our input size is 1.\n",
    "            #RNN module outputs final output and hidden state.  Since we are just working with one\n",
    "            #time step this is simply the next output and next hidden state\n",
    "            output, h = self.rnn(inp.reshape(1, 1, 1), h)\n",
    "            #Output will be of size (time_steps=1,batch_size=1,output_size=1) so we squeeze to get\n",
    "            #a single value.  Then we apply our linear transformation\n",
    "            output = self.linear(torch.squeeze(output))\n",
    "            #Append output to our list of outputs\n",
    "            outputs.append(output)\n",
    "        #Save final hidden state to return\n",
    "        final_h = h\n",
    "        \n",
    "        outputs = torch.squeeze(torch.cat(outputs))\n",
    "        \n",
    "        return outputs, final_h\n",
    "    \n",
    "    #Given a set of inputs runs through model collecting final hidden state\n",
    "    #Using hidden state we extrapolate exclusively feeding the output of the \n",
    "    #model back in to create a self sustaining model of the future\n",
    "    def extrapolate(self, inputs, num_futures):\n",
    "        #We don't need to keep track of gradients here (not used for learning)\n",
    "        with torch.no_grad():\n",
    "            #Get final hidden state of model after passing through given inputs\n",
    "            #As well as outputs over the course of the forward pass.\n",
    "            outputs, final_h = self.forward(inputs)\n",
    "            \n",
    "            #Initialize lists to contain future extrapolated points\n",
    "            future_points = []\n",
    "            #Initialize hidden state as final hidden state found earlier\n",
    "            h = final_h\n",
    "            #Last predicted point initialized to last point predicted by model\n",
    "            pred_pt = outputs[-1]\n",
    "            #For every point you want to extrapolate\n",
    "            for _ in range(num_futures):\n",
    "                #Output and next hidden state gotten through forward pass\n",
    "                pred, h = self.rnn(pred_pt.reshape(1, 1, 1), h)\n",
    "                #Apply linear transformation on output to get predicted point\n",
    "                pred_pt = self.linear(pred)\n",
    "                #Append predicted point to list of extrapolated points\n",
    "                future_points.append(pred_pt.item())\n",
    "            #Return an ndarray containing extrapolated points\n",
    "            return np.array(future_points)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Epochs\n",
    "EPOCHS = 20\n",
    "#Learning Rate\n",
    "LR = 0.04\n",
    "#Momentum Constant\n",
    "BETA = 0.2\n",
    "#Dimensionality of RNN Hidden State\n",
    "HIDDEN_SIZE = (16, 64)\n",
    "\n",
    "#Initialize network\n",
    "net = LSTM_RNN(HIDDEN_SIZE).double()\n",
    "\n",
    "#We will use Mean Squared Loss\n",
    "criterion = nn.MSELoss()\n",
    "#Optimize using Stochastic Gradient Descent\n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=BETA)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    #Value to display reset each epoch\n",
    "    total_loss = 0\n",
    "    #Online learning - Update weights after each example\n",
    "    for inputs, target in dataset:\n",
    "        #We are only interested in the predictions made by the model\n",
    "        #So only save the outputs given the inputs\n",
    "        output, _ = net(inputs)\n",
    "        #'Squeeze' the outputs to reduce to a single dimensional list of predictions\n",
    "        output = output.reshape(-1)\n",
    "        #Loss is the mean squared error with the expected targets\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        #Zero the gradients before computing new gradients\n",
    "        net.zero_grad()\n",
    "        #Compute gradients in a backward pass\n",
    "        loss.backward()\n",
    "        #Update the parameters to minimize loss\n",
    "        optimizer.step()\n",
    "        \n",
    "        #Increment the total loss for this Epoch\n",
    "        total_loss += loss\n",
    "    #Divide total loss by the length of the dataset to get 'average' total loss  \n",
    "    total_loss /= len(dataset)\n",
    "    #Display average loss for Epoch\n",
    "    print(f\"Epoch {i+1}: {total_loss.item()}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extrapolation Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of points we want to extrapolate\n",
    "num_future = 200\n",
    "\n",
    "#We will use the first example function in our dataset.  Note the second 0 is to retrieve\n",
    "#the points [y_0,y_1,...,y_(n-1)]] as we saved earlier. (dataset[i][1]) would get\n",
    "#[y_1,y_2,...,y_n])\n",
    "ground = dataset[np.random.randint(0,N)][0]\n",
    "#Number of points we are given\n",
    "num_ground = ground.shape[0]\n",
    "#Our extrapolated points\n",
    "prediction = net.extrapolate(ground, num_future)\n",
    "\n",
    "#Plot the extrapolated points along with the given input.\n",
    "plt.figure(figsize=(16, 9))\n",
    "plt.scatter(np.arange(0, num_ground), ground, s=20)\n",
    "plt.scatter(np.arange(num_ground, num_ground+num_future), prediction, s=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
