{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This homework is intended as a brief overview of the machine learning process and the various topics you will learn in this class. We hope that this exercise will allow you to put in context the information you learn with us this semester. Don't worry if you don't understand the techniques here (that's what you'll learn this semester!); we just want to show you how you can use sklearn to do simple machine learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us import some libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shutil import copyfileobj\n",
    "from six.moves import urllib\n",
    "from sklearn.datasets.base import get_data_home\n",
    "import os\n",
    "\n",
    "def fetch_mnist(data_home=None):\n",
    "    mnist_alternative_url = \"https://github.com/amplab/datascience-sp14/raw/master/lab7/mldata/mnist-original.mat\"\n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    data_home = os.path.join(data_home, 'mldata')\n",
    "    if not os.path.exists(data_home):\n",
    "        os.makedirs(data_home)\n",
    "    mnist_save_path = os.path.join(data_home, \"mnist-original.mat\")\n",
    "    if not os.path.exists(mnist_save_path):\n",
    "        mnist_url = urllib.request.urlopen(mnist_alternative_url)\n",
    "        with open(mnist_save_path, \"wb\") as matlab_file:\n",
    "            copyfileobj(mnist_url, matlab_file)\n",
    "fetch_mnist()\n",
    "from sklearn.datasets import fetch_mldata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function fetch_mldata is deprecated; fetch_mldata was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:77: DeprecationWarning: Function mldata_filename is deprecated; mldata_filename was deprecated in version 0.20 and will be removed in version 0.22\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "X = mnist.data.astype('float64')\n",
    "y = mnist.target.astype('int64')\n",
    "\n",
    "#from keras.datasets.mnist import load_data\n",
    "#(x_train, y_train), (x_test, y_test) = load_data()\n",
    "#X = np.concatenate([x_train, x_test]).reshape(-1, 784)\n",
    "#y = np.concatenate([y_train, y_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first explore this data a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(70000, 784) (70000,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X matrix here contains all the digit pictures. The data is (n_samples x n_features), meaning this data contains 70,000 pictures, each with 784 features (the 28x28 image is flattened into a single row). The y vector contains the label for each digit, so we know which digit (or class - class means category) is in each picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try and visualize this data a bit. Change around the index variable to explore more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f21cd82c7f0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0 #15000, 28999, 67345\n",
    "image = X[index].reshape((28, 28))\n",
    "plt.title('Label is ' + str(y[index]))\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that each pixel value ranges from 0-255. When we train our models, a good practice is to *standardize* the data so different features can be compared more equally. Here we will use a simple standardization, squeezing all values into the 0-1 interval range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we train our model, we want it to have the lowest error. Error presents itself in 2 ways: bias (how close our model is to the ideal model), and variance (how much our model varies with different datasets). If we train our model on a chunk of data, and then test our model on that same data, we will only witness the first type of error - bias. However, if we test on new, unseen data, that will reflect both bias and variance. This is the reasoning behind cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we want to have 2 datasets, train and test, each used for the named purpose exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will walk you through applying various models to try and achieve the lowest error rate on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our labels is a number from 0-9. If we simply did regression on this data, the labels would imply some sort of ordering of the classes (ie the digit 8 is more of the digit 7 than the digit 3 is, etc. We can fix this issue by one-hot encoding our labels. So, instead of each label being a simple digit, each label is a vector of 10 entries. 9 of those entries are zero, and only 1 entry is equal to one, corresponding to the index of the digit. Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:368: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(70000, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "y_hot = enc.fit_transform(y.reshape(-1, 1))\n",
    "y_train_hot = enc.transform(y_train.reshape(-1, 1))\n",
    "y_hot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember how the first sample is the digit zero? Let's now look at the new label at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hot[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 steps to build your model: create the model, train the model, then use your model to make predictions). In the sklearn API, this is made very clear. First you instantiate the model (constructor), then you call train it with the `fit` method, then you can make predictions on new data with the `test` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do a basic linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None,\n",
       "         normalize=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear = LinearRegression()\n",
    "linear.fit(X_train, y_train_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.8586857142857143\n",
      "test acc:  0.8541714285714286\n"
     ]
    }
   ],
   "source": [
    "# use trained model to predict both train and test sets\n",
    "y_train_pred = linear.predict(X_train)\n",
    "y_test_pred = linear.predict(X_test)\n",
    "\n",
    "# print accuracies\n",
    "print('train acc: ', accuracy_score(y_train_pred.argmax(axis=1), y_train))\n",
    "print('test acc: ', accuracy_score(y_test_pred.argmax(axis=1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on interpretability: you can view the weights of your model with `linear.coef_`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try and regularize by adding a penalty term to see if we can get anything better. We can penalize via the L2 norm, aka Ridge Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.8586666666666667\n",
      "test acc:  0.8545142857142857\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=0.15)\n",
    "ridge.fit(X_train, y_train_hot)\n",
    "print('train acc: ', accuracy_score(ridge.predict(X_train).argmax(axis=1), y_train))\n",
    "print('test acc: ', accuracy_score(ridge.predict(X_test).argmax(axis=1), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha controls how much to penalize the weights. Play around with it to see if you can improve the test accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen how to use some basic models to fit and evaluate your data. You will now walk through working with more models. Fill in code where needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now do logistic regression. From now on, the models will automatically one-hot the labels (so we don't need to worry about it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9286857142857143\n",
      "test acc:  0.9224571428571429\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=0.2, multi_class='multinomial', solver='saga', tol=0.1)\n",
    "logreg.fit(X_train, y_train)\n",
    "print('train acc: ', accuracy_score(logreg.predict(X_train), y_train))\n",
    "print('test acc: ', accuracy_score(logreg.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy has jumped ~5%! Why is this? Logistic Regression is a more complex model - instead of computing raw scores as in linear regression, it does one extra step and squashes values between 0 and 1. This means our model now optimizes over *probabilities* instead of raw scores. This makes sense since our vectors are 1-hot encoded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The C hyperparameter controls inverse regularization strength (inverse for this model only). Reguralization is important to make sure our model doesn't overfit (perform much better on train data than test data). Play around with the C parameter to try and get better results! You should be able to hit 92%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Trees are a completely different type of classifier. They essentially break up the possible space by repeatedly \"splitting\" on features to keep narrowing down the possibilities. Decision Trees are normally individually very week, so we typically average them together in bunches called Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have seen many examples for how to construct, fit, and evaluate a model. Now do the same for Random Forest using the [documentation here](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html). You should be able to create one easily without needing to specify any constructor parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  1.0\n",
      "text acc:  0.9664\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=100, min_samples_split=2, max_features=8)\n",
    "rf.fit(X_train, y_train)\n",
    "print('train acc: ', accuracy_score(rf.predict(X_train), y_train))\n",
    "print('text acc: ', accuracy_score(rf.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WOWZA! That train accuracy is amazing, let's see if we can boost up the test accuracy a bit (since that's what really counts). Try and play around with the hyperparameters to see if you can edge out more accuracy (look at the documentation for parameters in the constructor). Focus on `n_estimators`, `min_samples_split`, `max_features`. You should be able to hit ~97%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A support vector classifier is another completely different type of classifier. It tries to find the best separating hyperplane through your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVC will toast our laptops unless we reduce the data dimensionality. Let's keep 80% of the variation, and get rid of the rest. (This will cause a slight drop in performance, but not by much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.8, whiten=True)\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now let's take a look at what that actually did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(52500, 43)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, before we had 784 (28x28) features! However, PCA found just 43 basis features that explain 80% of the data. So, we went to just 5% of the original input space, but we still retained 80% of the information! Nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This [blog post](http://colah.github.io/posts/2014-10-Visualizing-MNIST/) explains dimensionality reduction with MNIST far better than I can. It's a short read (<10 mins), and it contains some pretty cool visualizations. Read it and jot down things you learned from the post or further questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* \"We need to decide which direction every axis of the cube should be tilted\" ... This rattles my brain!  How does tilting a 784-dimensional cube play a part in the PCA process?  The article didn't quite make that clear.\n",
    "* Abtractly, I understand how, using PCA, we can place images on the described red/blue scale.  I'd like to know how we select these \"regions\" (or cube rotation?) to generate a scale that appopriately groups records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train our first SVC. The LinearSVC can only find a linear decision boundary (the hyperplane)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.8927238095238095\n",
      "test acc:  0.8934285714285715\n"
     ]
    }
   ],
   "source": [
    "lsvc = LinearSVC(dual=False, tol=0.01)\n",
    "lsvc.fit(X_train_pca, y_train)\n",
    "print('train acc: ', accuracy_score(lsvc.predict(X_train_pca), y_train))\n",
    "print('test acc: ', accuracy_score(lsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs are really interesting because they have something called the *dual formulation*, in which the computation is expressed as training point inner products. This means that data can be lifted into higher dimensions easily with this \"kernel trick\". Data that is not linearly separable in a lower dimension can be linearly separable in a higher dimension - which is why we conduct the transform. Let us experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A transformation that lifts the data into a higher-dimensional space is called a kernel. A polynomial kernel expands the feature space by computing all the polynomial cross terms to a specific degree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.9961904761904762\n",
      "test acc:  0.9821142857142857\n"
     ]
    }
   ],
   "source": [
    "psvc = SVC(kernel='poly', degree=3, tol=0.01, cache_size=4000)\n",
    "## YOUR CODE HERE - fit the psvc model\n",
    "psvc.fit(X_train_pca, y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "print('train acc: ', accuracy_score(psvc.predict(X_train_pca), y_train))\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('test acc: ', accuracy_score(psvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with the degree of the polynomial kernel to see what accuracy you can get."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RBF kernel uses the gaussian function to create an infinite dimensional space - a gaussian peak at each datapoint. Now fiddle with the `C` and `gamma` parameters of the gaussian kernel below to see what you can get. [Here's documentation](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train acc:  0.992647619047619\n",
      "test acc:  0.9825714285714285\n"
     ]
    }
   ],
   "source": [
    "rsvc = SVC(kernel='rbf', tol=0.01, cache_size=4000)\n",
    "## YOUR CODE HERE - fit the rsvc model\n",
    "rsvc.fit(X_train_pca, y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "print('train acc: ', accuracy_score(rsvc.predict(X_train_pca), y_train))\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('test acc: ', accuracy_score(rsvc.predict(X_test_pca), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isn't that just amazing accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should never do neural networks in sklearn. Use Keras (which we will teach you later in this class), Tensorflow, PyTorch, etc. However, in an effort to keep this homework somewhat cohesive, let us proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic neural networks proceed in layers. Each layer has a certain number of nodes, representing how expressive that layer can be. Below is a sample network, with an input layer, one hidden (middle) layer of 50 neurons, and finally the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.38962066\n",
      "Iteration 2, loss = 0.17448919\n",
      "Iteration 3, loss = 0.12301024\n",
      "Iteration 4, loss = 0.09437451\n",
      "Iteration 5, loss = 0.07519934\n",
      "Iteration 6, loss = 0.06022895\n",
      "Iteration 7, loss = 0.05040258\n",
      "Iteration 8, loss = 0.04186103\n",
      "Iteration 9, loss = 0.03473548\n",
      "Iteration 10, loss = 0.02887468\n",
      "Iteration 11, loss = 0.02422830\n",
      "Iteration 12, loss = 0.02093050\n",
      "Iteration 13, loss = 0.01699150\n",
      "Iteration 14, loss = 0.01419818\n",
      "Iteration 15, loss = 0.01234327\n",
      "Iteration 16, loss = 0.01067494\n",
      "Iteration 17, loss = 0.00876178\n",
      "Iteration 18, loss = 0.00704831\n",
      "Iteration 19, loss = 0.00603213\n",
      "Iteration 20, loss = 0.00530036\n",
      "Iteration 21, loss = 0.00451163\n",
      "Iteration 22, loss = 0.00492090\n",
      "Iteration 23, loss = 0.00496265\n",
      "Iteration 24, loss = 0.00454364\n",
      "Iteration 25, loss = 0.00505443\n",
      "Iteration 26, loss = 0.00351642\n",
      "Iteration 27, loss = 0.00208908\n",
      "Iteration 28, loss = 0.00157696\n",
      "Iteration 29, loss = 0.00136557\n",
      "Iteration 30, loss = 0.00124956\n",
      "Iteration 31, loss = 0.00118595\n",
      "Iteration 32, loss = 0.00112293\n",
      "Iteration 33, loss = 0.00106864\n",
      "Iteration 34, loss = 0.00100961\n",
      "Iteration 35, loss = 0.00095227\n",
      "Iteration 36, loss = 0.00091255\n",
      "Iteration 37, loss = 0.00087600\n",
      "Iteration 38, loss = 0.01122046\n",
      "Iteration 39, loss = 0.01230409\n",
      "Iteration 40, loss = 0.00263207\n",
      "Iteration 41, loss = 0.00114032\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "train acc:  1.0\n",
      "test acc:  0.9811428571428571\n"
     ]
    }
   ],
   "source": [
    "nn = MLPClassifier(hidden_layer_sizes=(200,), solver='adam', verbose=1)\n",
    "## YOUR CODE HERE - fit the nn\n",
    "nn.fit(X_train, y_train)\n",
    "## YOUR CODE HERE - print training accuracy\n",
    "print('train acc: ', accuracy_score(nn.predict(X_train), y_train))\n",
    "## YOUR CODE HERE - print test accuracy\n",
    "print('test acc: ', accuracy_score(nn.predict(X_test), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fiddle around with the hiddle layers. Change the number of neurons, add more layers, experiment. You should be able to hit 98% accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks are optimized with a technique called gradient descent (a neural net is just one big function - so we can take the gradient with respect to all its parameters, then just go opposite the gradient to try and find the minimum). This is why it requires many iterations to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning In"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert this notebook to a PDF (file -> download as -> pdf via latex) and submit to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
